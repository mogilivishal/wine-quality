{"nbformat_minor": 2, "cells": [{"source": "import pandas as pd\nimport numpy as np\nimport os\nos.getcwd()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "'/home/dsxuser/work'"}, "execution_count": 1, "metadata": {}}], "execution_count": 1}, {"source": "\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share your notebook.\nclient_fee561139b3741cb9557e4db160e1832 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='CGpSDGdRcbRQr5hgo_7dkSijweyGKbxRsben4G04E6Kn',\n    ibm_auth_endpoint=\"https://iam.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.eu-geo.objectstorage.service.networklayer.com')\n\nbody = client_fee561139b3741cb9557e4db160e1832.get_object(Bucket='deeplearningprojects-donotdelete-pr-90fl5u3omzuyo7',Key='winequality-white.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf = pd.read_csv(body,delimiter=\";\")\ndf.head()\n\n", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.0</td>\n      <td>0.27</td>\n      <td>0.36</td>\n      <td>20.7</td>\n      <td>0.045</td>\n      <td>45.0</td>\n      <td>170.0</td>\n      <td>1.0010</td>\n      <td>3.00</td>\n      <td>0.45</td>\n      <td>8.8</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6.3</td>\n      <td>0.30</td>\n      <td>0.34</td>\n      <td>1.6</td>\n      <td>0.049</td>\n      <td>14.0</td>\n      <td>132.0</td>\n      <td>0.9940</td>\n      <td>3.30</td>\n      <td>0.49</td>\n      <td>9.5</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.1</td>\n      <td>0.28</td>\n      <td>0.40</td>\n      <td>6.9</td>\n      <td>0.050</td>\n      <td>30.0</td>\n      <td>97.0</td>\n      <td>0.9951</td>\n      <td>3.26</td>\n      <td>0.44</td>\n      <td>10.1</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7.2</td>\n      <td>0.23</td>\n      <td>0.32</td>\n      <td>8.5</td>\n      <td>0.058</td>\n      <td>47.0</td>\n      <td>186.0</td>\n      <td>0.9956</td>\n      <td>3.19</td>\n      <td>0.40</td>\n      <td>9.9</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.2</td>\n      <td>0.23</td>\n      <td>0.32</td>\n      <td>8.5</td>\n      <td>0.058</td>\n      <td>47.0</td>\n      <td>186.0</td>\n      <td>0.9956</td>\n      <td>3.19</td>\n      <td>0.40</td>\n      <td>9.9</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0            7.0              0.27         0.36            20.7      0.045   \n1            6.3              0.30         0.34             1.6      0.049   \n2            8.1              0.28         0.40             6.9      0.050   \n3            7.2              0.23         0.32             8.5      0.058   \n4            7.2              0.23         0.32             8.5      0.058   \n\n   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n0                 45.0                 170.0   1.0010  3.00       0.45   \n1                 14.0                 132.0   0.9940  3.30       0.49   \n2                 30.0                  97.0   0.9951  3.26       0.44   \n3                 47.0                 186.0   0.9956  3.19       0.40   \n4                 47.0                 186.0   0.9956  3.19       0.40   \n\n   alcohol  quality  \n0      8.8        6  \n1      9.5        6  \n2     10.1        6  \n3      9.9        6  \n4      9.9        6  "}, "execution_count": 4, "metadata": {}}], "execution_count": 4}, {"source": "df.head()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.0</td>\n      <td>0.27</td>\n      <td>0.36</td>\n      <td>20.7</td>\n      <td>0.045</td>\n      <td>45.0</td>\n      <td>170.0</td>\n      <td>1.0010</td>\n      <td>3.00</td>\n      <td>0.45</td>\n      <td>8.8</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6.3</td>\n      <td>0.30</td>\n      <td>0.34</td>\n      <td>1.6</td>\n      <td>0.049</td>\n      <td>14.0</td>\n      <td>132.0</td>\n      <td>0.9940</td>\n      <td>3.30</td>\n      <td>0.49</td>\n      <td>9.5</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.1</td>\n      <td>0.28</td>\n      <td>0.40</td>\n      <td>6.9</td>\n      <td>0.050</td>\n      <td>30.0</td>\n      <td>97.0</td>\n      <td>0.9951</td>\n      <td>3.26</td>\n      <td>0.44</td>\n      <td>10.1</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7.2</td>\n      <td>0.23</td>\n      <td>0.32</td>\n      <td>8.5</td>\n      <td>0.058</td>\n      <td>47.0</td>\n      <td>186.0</td>\n      <td>0.9956</td>\n      <td>3.19</td>\n      <td>0.40</td>\n      <td>9.9</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.2</td>\n      <td>0.23</td>\n      <td>0.32</td>\n      <td>8.5</td>\n      <td>0.058</td>\n      <td>47.0</td>\n      <td>186.0</td>\n      <td>0.9956</td>\n      <td>3.19</td>\n      <td>0.40</td>\n      <td>9.9</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0            7.0              0.27         0.36            20.7      0.045   \n1            6.3              0.30         0.34             1.6      0.049   \n2            8.1              0.28         0.40             6.9      0.050   \n3            7.2              0.23         0.32             8.5      0.058   \n4            7.2              0.23         0.32             8.5      0.058   \n\n   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n0                 45.0                 170.0   1.0010  3.00       0.45   \n1                 14.0                 132.0   0.9940  3.30       0.49   \n2                 30.0                  97.0   0.9951  3.26       0.44   \n3                 47.0                 186.0   0.9956  3.19       0.40   \n4                 47.0                 186.0   0.9956  3.19       0.40   \n\n   alcohol  quality  \n0      8.8        6  \n1      9.5        6  \n2     10.1        6  \n3      9.9        6  \n4      9.9        6  "}, "execution_count": 5, "metadata": {}}], "execution_count": 5}, {"source": "x = df.iloc[:,0:11].values", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 6}, {"source": "y=df.iloc[:,11].values\n\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 7}, {"source": "x", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([[  7.  ,   0.27,   0.36, ...,   3.  ,   0.45,   8.8 ],\n       [  6.3 ,   0.3 ,   0.34, ...,   3.3 ,   0.49,   9.5 ],\n       [  8.1 ,   0.28,   0.4 , ...,   3.26,   0.44,  10.1 ],\n       ..., \n       [  6.5 ,   0.24,   0.19, ...,   2.99,   0.46,   9.4 ],\n       [  5.5 ,   0.29,   0.3 , ...,   3.34,   0.38,  12.8 ],\n       [  6.  ,   0.21,   0.38, ...,   3.26,   0.32,  11.8 ]])"}, "execution_count": 8, "metadata": {}}], "execution_count": 8}, {"source": "y\n\n\n\n\n", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([6, 6, 6, ..., 6, 7, 6])"}, "execution_count": 9, "metadata": {}}], "execution_count": 9}, {"source": "from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=0)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 10}, {"source": "from sklearn.preprocessing import StandardScaler  \nsc=StandardScaler()\nx_train = sc.fit_transform(x_train)  ## finding out the best value and transforming\nx_test = sc.transform(x_test) ", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 11}, {"source": "from keras.utils import to_categorical\ny_train = to_categorical(y_train)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "Using TensorFlow backend.\n"}], "execution_count": 12}, {"source": "import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 13}, {"source": "", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}, {"source": "\nmodel=Sequential()\nmodel.add(Dense(10,input_dim=11))\nmodel.add(Dense(100,activation = 'relu'))\nmodel.add(Dense(100,activation = 'relu'))\nmodel.add(Dense(100,activation = 'relu'))\nmodel.add(Dense(10,activation = 'softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy']) \n       # for regression no need of activation function\n", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 14}, {"source": "model.fit(x_train,y_train,epochs= 100,batch_size=32)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Epoch 1/100\n3918/3918 [==============================] - 1s 239us/step - loss: 1.3396 - acc: 0.4801\nEpoch 2/100\n3918/3918 [==============================] - 1s 206us/step - loss: 1.0911 - acc: 0.5408\nEpoch 3/100\n3918/3918 [==============================] - 1s 204us/step - loss: 1.0564 - acc: 0.5544\nEpoch 4/100\n3918/3918 [==============================] - 1s 206us/step - loss: 1.0358 - acc: 0.5620\nEpoch 5/100\n3918/3918 [==============================] - 1s 204us/step - loss: 1.0115 - acc: 0.5745\nEpoch 6/100\n3918/3918 [==============================] - 1s 203us/step - loss: 0.9923 - acc: 0.5817\nEpoch 7/100\n3918/3918 [==============================] - 1s 205us/step - loss: 0.9784 - acc: 0.5789\nEpoch 8/100\n3918/3918 [==============================] - 1s 184us/step - loss: 0.9625 - acc: 0.5909\nEpoch 9/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.9491 - acc: 0.5965\nEpoch 10/100\n3918/3918 [==============================] - 1s 203us/step - loss: 0.9304 - acc: 0.6064\nEpoch 11/100\n3918/3918 [==============================] - 1s 184us/step - loss: 0.9183 - acc: 0.6182\nEpoch 12/100\n3918/3918 [==============================] - 1s 204us/step - loss: 0.9100 - acc: 0.6118\nEpoch 13/100\n3918/3918 [==============================] - 1s 203us/step - loss: 0.8955 - acc: 0.6223\nEpoch 14/100\n3918/3918 [==============================] - 1s 204us/step - loss: 0.8792 - acc: 0.6266\nEpoch 15/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.8708 - acc: 0.6371\nEpoch 16/100\n3918/3918 [==============================] - 1s 207us/step - loss: 0.8548 - acc: 0.6401\nEpoch 17/100\n3918/3918 [==============================] - 1s 201us/step - loss: 0.8354 - acc: 0.6475\nEpoch 18/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.8280 - acc: 0.6575\nEpoch 19/100\n3918/3918 [==============================] - 1s 201us/step - loss: 0.8133 - acc: 0.6641\nEpoch 20/100\n3918/3918 [==============================] - 1s 185us/step - loss: 0.7934 - acc: 0.6710\nEpoch 21/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.7853 - acc: 0.6774\nEpoch 22/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.7694 - acc: 0.6748\nEpoch 23/100\n3918/3918 [==============================] - 1s 204us/step - loss: 0.7583 - acc: 0.6909\nEpoch 24/100\n3918/3918 [==============================] - 1s 203us/step - loss: 0.7380 - acc: 0.7052\nEpoch 25/100\n3918/3918 [==============================] - 1s 182us/step - loss: 0.7357 - acc: 0.6991\nEpoch 26/100\n3918/3918 [==============================] - 1s 203us/step - loss: 0.7231 - acc: 0.7034 0s - loss: 0.7250 - acc: 0\nEpoch 27/100\n3918/3918 [==============================] - 1s 200us/step - loss: 0.7034 - acc: 0.7141\nEpoch 28/100\n3918/3918 [==============================] - 1s 184us/step - loss: 0.6899 - acc: 0.7182\nEpoch 29/100\n3918/3918 [==============================] - 1s 203us/step - loss: 0.6802 - acc: 0.7300\nEpoch 30/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.6610 - acc: 0.7361\nEpoch 31/100\n3918/3918 [==============================] - 1s 200us/step - loss: 0.6425 - acc: 0.7409\nEpoch 32/100\n3918/3918 [==============================] - 1s 185us/step - loss: 0.6288 - acc: 0.7496 0s - loss: 0.6340 - acc: 0.7\nEpoch 33/100\n3918/3918 [==============================] - 1s 204us/step - loss: 0.6217 - acc: 0.7501\nEpoch 34/100\n3918/3918 [==============================] - 1s 204us/step - loss: 0.6055 - acc: 0.7611\nEpoch 35/100\n3918/3918 [==============================] - 1s 224us/step - loss: 0.5921 - acc: 0.7596\nEpoch 36/100\n3918/3918 [==============================] - 1s 203us/step - loss: 0.5824 - acc: 0.7688\nEpoch 37/100\n3918/3918 [==============================] - 1s 186us/step - loss: 0.5851 - acc: 0.7639\nEpoch 38/100\n3918/3918 [==============================] - 1s 222us/step - loss: 0.5427 - acc: 0.7861\nEpoch 39/100\n3918/3918 [==============================] - 1s 186us/step - loss: 0.5285 - acc: 0.7925\nEpoch 40/100\n3918/3918 [==============================] - 1s 201us/step - loss: 0.5301 - acc: 0.7861\nEpoch 41/100\n3918/3918 [==============================] - 1s 203us/step - loss: 0.5077 - acc: 0.8004\nEpoch 42/100\n3918/3918 [==============================] - 1s 203us/step - loss: 0.4947 - acc: 0.8050\nEpoch 43/100\n3918/3918 [==============================] - 1s 204us/step - loss: 0.4818 - acc: 0.8119\nEpoch 44/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.4720 - acc: 0.8160\nEpoch 45/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.4529 - acc: 0.8252\nEpoch 46/100\n3918/3918 [==============================] - 1s 203us/step - loss: 0.4463 - acc: 0.8267\nEpoch 47/100\n3918/3918 [==============================] - 1s 185us/step - loss: 0.4388 - acc: 0.8341\nEpoch 48/100\n3918/3918 [==============================] - 1s 203us/step - loss: 0.4234 - acc: 0.8346\nEpoch 49/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.4249 - acc: 0.8331\nEpoch 50/100\n3918/3918 [==============================] - 1s 183us/step - loss: 0.3961 - acc: 0.8504 0s - loss: 0.3860 - acc: \nEpoch 51/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.3942 - acc: 0.8484\nEpoch 52/100\n3918/3918 [==============================] - 1s 201us/step - loss: 0.3746 - acc: 0.8622\nEpoch 53/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.3641 - acc: 0.8520\nEpoch 54/100\n3918/3918 [==============================] - 1s 204us/step - loss: 0.3473 - acc: 0.8721\nEpoch 55/100\n3918/3918 [==============================] - 1s 185us/step - loss: 0.3423 - acc: 0.8698 0s - loss: 0.3355 - acc: 0.873\nEpoch 56/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.3608 - acc: 0.8594\nEpoch 57/100\n3918/3918 [==============================] - 1s 201us/step - loss: 0.3190 - acc: 0.8836\nEpoch 58/100\n3918/3918 [==============================] - 1s 203us/step - loss: 0.3048 - acc: 0.8887\nEpoch 59/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.3114 - acc: 0.8790\nEpoch 60/100\n3918/3918 [==============================] - 1s 184us/step - loss: 0.3045 - acc: 0.8851\nEpoch 61/100\n3918/3918 [==============================] - 1s 201us/step - loss: 0.2783 - acc: 0.8999\nEpoch 62/100\n3918/3918 [==============================] - 1s 201us/step - loss: 0.2829 - acc: 0.8969\nEpoch 63/100\n3918/3918 [==============================] - 1s 185us/step - loss: 0.2673 - acc: 0.9068\nEpoch 64/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.2853 - acc: 0.8890\nEpoch 65/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.2653 - acc: 0.9010\nEpoch 66/100\n3918/3918 [==============================] - 1s 203us/step - loss: 0.2459 - acc: 0.9135\nEpoch 67/100\n3918/3918 [==============================] - 1s 183us/step - loss: 0.2366 - acc: 0.9135\nEpoch 68/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.2434 - acc: 0.9102\nEpoch 69/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.2308 - acc: 0.9170\nEpoch 70/100\n3918/3918 [==============================] - 1s 203us/step - loss: 0.2193 - acc: 0.9252\nEpoch 71/100\n3918/3918 [==============================] - 1s 204us/step - loss: 0.2230 - acc: 0.9170 1s - loss: 0.2196 - acc\nEpoch 72/100\n3918/3918 [==============================] - 1s 204us/step - loss: 0.2035 - acc: 0.9296\nEpoch 73/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.2038 - acc: 0.9313\nEpoch 74/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.2234 - acc: 0.9204\nEpoch 75/100\n3918/3918 [==============================] - 1s 206us/step - loss: 0.2226 - acc: 0.9153\nEpoch 76/100\n3918/3918 [==============================] - 1s 185us/step - loss: 0.1969 - acc: 0.9331\nEpoch 77/100\n3918/3918 [==============================] - 1s 200us/step - loss: 0.1754 - acc: 0.9428\nEpoch 78/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.1851 - acc: 0.9347\nEpoch 79/100\n3918/3918 [==============================] - 1s 184us/step - loss: 0.1641 - acc: 0.9433\nEpoch 80/100\n"}, {"output_type": "stream", "name": "stdout", "text": "3918/3918 [==============================] - 1s 201us/step - loss: 0.1716 - acc: 0.9438\nEpoch 81/100\n3918/3918 [==============================] - 1s 203us/step - loss: 0.1752 - acc: 0.9375\nEpoch 82/100\n3918/3918 [==============================] - 1s 204us/step - loss: 0.1495 - acc: 0.9507\nEpoch 83/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.1501 - acc: 0.9530\nEpoch 84/100\n3918/3918 [==============================] - 1s 183us/step - loss: 0.1547 - acc: 0.9497\nEpoch 85/100\n3918/3918 [==============================] - 1s 201us/step - loss: 0.1527 - acc: 0.9518\nEpoch 86/100\n3918/3918 [==============================] - 1s 203us/step - loss: 0.1428 - acc: 0.9546\nEpoch 87/100\n3918/3918 [==============================] - 1s 206us/step - loss: 0.1568 - acc: 0.9497\nEpoch 88/100\n3918/3918 [==============================] - 1s 201us/step - loss: 0.1660 - acc: 0.9433\nEpoch 89/100\n3918/3918 [==============================] - 1s 184us/step - loss: 0.1496 - acc: 0.9502\nEpoch 90/100\n3918/3918 [==============================] - 1s 201us/step - loss: 0.1450 - acc: 0.9502\nEpoch 91/100\n3918/3918 [==============================] - 1s 201us/step - loss: 0.1292 - acc: 0.9604\nEpoch 92/100\n3918/3918 [==============================] - 1s 204us/step - loss: 0.1136 - acc: 0.9645\nEpoch 93/100\n3918/3918 [==============================] - 1s 204us/step - loss: 0.1215 - acc: 0.9663\nEpoch 94/100\n3918/3918 [==============================] - 1s 204us/step - loss: 0.1465 - acc: 0.9510\nEpoch 95/100\n3918/3918 [==============================] - 1s 202us/step - loss: 0.1880 - acc: 0.9400 0s - loss: 0.2066 - acc: \nEpoch 96/100\n3918/3918 [==============================] - 1s 201us/step - loss: 0.1147 - acc: 0.9632\nEpoch 97/100\n3918/3918 [==============================] - 1s 185us/step - loss: 0.1099 - acc: 0.9691\nEpoch 98/100\n3918/3918 [==============================] - 1s 203us/step - loss: 0.1110 - acc: 0.9676\nEpoch 99/100\n3918/3918 [==============================] - 1s 204us/step - loss: 0.1099 - acc: 0.9658\nEpoch 100/100\n3918/3918 [==============================] - 1s 201us/step - loss: 0.0981 - acc: 0.9701\n"}, {"output_type": "execute_result", "data": {"text/plain": "<keras.callbacks.History at 0x7fa70a225748>"}, "execution_count": 15, "metadata": {}}], "execution_count": 15}, {"source": "y_train", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n       ..., \n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"}, "execution_count": 16, "metadata": {}}], "execution_count": 16}, {"source": "#y_pred = model.predict(x_train[0:1])", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 17}, {"source": "y_pred = model.predict_classes(x_train[0:1])", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 18}, {"source": "y_pred", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([5])"}, "execution_count": 19, "metadata": {}}], "execution_count": 19}, {"source": "x_train[0:1]", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([[ 0.41960224, -0.18345438, -0.10617414,  0.80769346,  0.73339424,\n        -0.73160893, -0.57903849,  0.88522816,  0.27871789, -0.0037976 ,\n        -0.01506672]])"}, "execution_count": 20, "metadata": {}}], "execution_count": 20}, {"source": "df.quality.value_counts()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "6    2198\n5    1457\n7     880\n8     175\n4     163\n3      20\n9       5\nName: quality, dtype: int64"}, "execution_count": 21, "metadata": {}}], "execution_count": 21}, {"source": "model.save(\"wine_quality.h5\")", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 22}, {"source": "!tar -zcvf wine_quality.tgz wine_quality.h5", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "wine_quality.h5\r\n"}], "execution_count": 23}, {"source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n"}], "execution_count": 24}, {"source": "wml_credentials = {\n    \"access_key\":\"NQwIDM3NeEfIQoBpKqSPOZQ6EGxzBVY0U0vYe6kSGJIr\",\n    \"instance_id\": \"010cf8d0-4738-4f77-b380-f1e9b70da3e2\",\n    \"password\": \"18dce468-8c59-48c3-82f2-de3cd105e1bf\",\n    \"url\": \"https://eu-gb.ml.cloud.ibm.com\",\n    \"username\": \"a95be078-3e88-422d-9394-cee3c9134cf7\"\n   \n}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 25}, {"source": "client = WatsonMachineLearningAPIClient(wml_credentials)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 26}, {"source": "metadata = {\n    client.repository.ModelMetaNames.NAME : \"keras model\",\n    client.repository.ModelMetaNames.FRAMEWORK_LIBRARIES: [{'name':'keras','version': '2.1.3'}],\n    client.repository.ModelMetaNames.FRAMEWORK_NAME : \"tensorflow\",\n    client.repository.ModelMetaNames.FRAMEWORK_VERSION : \"1.5\",\n}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 27}, {"source": "model_details = client.repository.store_model(model=\"wine_quality.tgz\",meta_props=metadata)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 28}, {"source": "client.repository.list_models()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "------------------------------------  -----------  ------------------------  --------------\nGUID                                  NAME         CREATED                   FRAMEWORK\n184bbce1-83a4-4ce0-b1c8-948fe4937097  keras model  2019-05-28T17:28:08.818Z  tensorflow-1.5\ne9f3f2eb-5b5b-4b14-848f-9f4714f61789  keras model  2019-05-28T16:50:34.313Z  tensorflow-1.5\ne7d80e18-a30c-4e5d-9161-2152ea65a93a  keras model  2019-05-17T10:19:32.949Z  tensorflow-1.5\n2649b526-4cda-4393-abd5-e849d56f567b  keras model  2019-05-17T10:02:47.168Z  tensorflow-1.5\n------------------------------------  -----------  ------------------------  --------------\n"}], "execution_count": 29}, {"source": "model_uid = model_details['metadata']['guid']\nmodel_deploy = client.deployments.create(artifact_uid=model_uid,name=\"wine-quality\")", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "\n\n#######################################################################################\n\nSynchronous deployment creation for uid: '184bbce1-83a4-4ce0-b1c8-948fe4937097' started\n\n#######################################################################################\n\n\nINITIALIZING\nDEPLOY_IN_PROGRESS\nDEPLOY_SUCCESS\n\n\n------------------------------------------------------------------------------------------------\nSuccessfully finished deployment creation, deployment_uid='7046a5e0-7daf-4d5f-aa91-8e80610df58d'\n------------------------------------------------------------------------------------------------\n\n\n"}], "execution_count": 30}, {"source": "scoring_endpoint = client.deployments.get_scoring_url(model_deploy)\nscoring_endpoint", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "'https://eu-gb.ml.cloud.ibm.com/v3/wml_instances/010cf8d0-4738-4f77-b380-f1e9b70da3e2/deployments/7046a5e0-7daf-4d5f-aa91-8e80610df58d/online'"}, "execution_count": 31, "metadata": {}}], "execution_count": 31}, {"source": "df", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.0</td>\n      <td>0.270</td>\n      <td>0.36</td>\n      <td>20.70</td>\n      <td>0.045</td>\n      <td>45.0</td>\n      <td>170.0</td>\n      <td>1.00100</td>\n      <td>3.00</td>\n      <td>0.45</td>\n      <td>8.800000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6.3</td>\n      <td>0.300</td>\n      <td>0.34</td>\n      <td>1.60</td>\n      <td>0.049</td>\n      <td>14.0</td>\n      <td>132.0</td>\n      <td>0.99400</td>\n      <td>3.30</td>\n      <td>0.49</td>\n      <td>9.500000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.1</td>\n      <td>0.280</td>\n      <td>0.40</td>\n      <td>6.90</td>\n      <td>0.050</td>\n      <td>30.0</td>\n      <td>97.0</td>\n      <td>0.99510</td>\n      <td>3.26</td>\n      <td>0.44</td>\n      <td>10.100000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7.2</td>\n      <td>0.230</td>\n      <td>0.32</td>\n      <td>8.50</td>\n      <td>0.058</td>\n      <td>47.0</td>\n      <td>186.0</td>\n      <td>0.99560</td>\n      <td>3.19</td>\n      <td>0.40</td>\n      <td>9.900000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.2</td>\n      <td>0.230</td>\n      <td>0.32</td>\n      <td>8.50</td>\n      <td>0.058</td>\n      <td>47.0</td>\n      <td>186.0</td>\n      <td>0.99560</td>\n      <td>3.19</td>\n      <td>0.40</td>\n      <td>9.900000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8.1</td>\n      <td>0.280</td>\n      <td>0.40</td>\n      <td>6.90</td>\n      <td>0.050</td>\n      <td>30.0</td>\n      <td>97.0</td>\n      <td>0.99510</td>\n      <td>3.26</td>\n      <td>0.44</td>\n      <td>10.100000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6.2</td>\n      <td>0.320</td>\n      <td>0.16</td>\n      <td>7.00</td>\n      <td>0.045</td>\n      <td>30.0</td>\n      <td>136.0</td>\n      <td>0.99490</td>\n      <td>3.18</td>\n      <td>0.47</td>\n      <td>9.600000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7.0</td>\n      <td>0.270</td>\n      <td>0.36</td>\n      <td>20.70</td>\n      <td>0.045</td>\n      <td>45.0</td>\n      <td>170.0</td>\n      <td>1.00100</td>\n      <td>3.00</td>\n      <td>0.45</td>\n      <td>8.800000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>6.3</td>\n      <td>0.300</td>\n      <td>0.34</td>\n      <td>1.60</td>\n      <td>0.049</td>\n      <td>14.0</td>\n      <td>132.0</td>\n      <td>0.99400</td>\n      <td>3.30</td>\n      <td>0.49</td>\n      <td>9.500000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8.1</td>\n      <td>0.220</td>\n      <td>0.43</td>\n      <td>1.50</td>\n      <td>0.044</td>\n      <td>28.0</td>\n      <td>129.0</td>\n      <td>0.99380</td>\n      <td>3.22</td>\n      <td>0.45</td>\n      <td>11.000000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>8.1</td>\n      <td>0.270</td>\n      <td>0.41</td>\n      <td>1.45</td>\n      <td>0.033</td>\n      <td>11.0</td>\n      <td>63.0</td>\n      <td>0.99080</td>\n      <td>2.99</td>\n      <td>0.56</td>\n      <td>12.000000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>8.6</td>\n      <td>0.230</td>\n      <td>0.40</td>\n      <td>4.20</td>\n      <td>0.035</td>\n      <td>17.0</td>\n      <td>109.0</td>\n      <td>0.99470</td>\n      <td>3.14</td>\n      <td>0.53</td>\n      <td>9.700000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>7.9</td>\n      <td>0.180</td>\n      <td>0.37</td>\n      <td>1.20</td>\n      <td>0.040</td>\n      <td>16.0</td>\n      <td>75.0</td>\n      <td>0.99200</td>\n      <td>3.18</td>\n      <td>0.63</td>\n      <td>10.800000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>6.6</td>\n      <td>0.160</td>\n      <td>0.40</td>\n      <td>1.50</td>\n      <td>0.044</td>\n      <td>48.0</td>\n      <td>143.0</td>\n      <td>0.99120</td>\n      <td>3.54</td>\n      <td>0.52</td>\n      <td>12.400000</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>8.3</td>\n      <td>0.420</td>\n      <td>0.62</td>\n      <td>19.25</td>\n      <td>0.040</td>\n      <td>41.0</td>\n      <td>172.0</td>\n      <td>1.00020</td>\n      <td>2.98</td>\n      <td>0.67</td>\n      <td>9.700000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>6.6</td>\n      <td>0.170</td>\n      <td>0.38</td>\n      <td>1.50</td>\n      <td>0.032</td>\n      <td>28.0</td>\n      <td>112.0</td>\n      <td>0.99140</td>\n      <td>3.25</td>\n      <td>0.55</td>\n      <td>11.400000</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>6.3</td>\n      <td>0.480</td>\n      <td>0.04</td>\n      <td>1.10</td>\n      <td>0.046</td>\n      <td>30.0</td>\n      <td>99.0</td>\n      <td>0.99280</td>\n      <td>3.24</td>\n      <td>0.36</td>\n      <td>9.600000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>6.2</td>\n      <td>0.660</td>\n      <td>0.48</td>\n      <td>1.20</td>\n      <td>0.029</td>\n      <td>29.0</td>\n      <td>75.0</td>\n      <td>0.98920</td>\n      <td>3.33</td>\n      <td>0.39</td>\n      <td>12.800000</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>7.4</td>\n      <td>0.340</td>\n      <td>0.42</td>\n      <td>1.10</td>\n      <td>0.033</td>\n      <td>17.0</td>\n      <td>171.0</td>\n      <td>0.99170</td>\n      <td>3.12</td>\n      <td>0.53</td>\n      <td>11.300000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>6.5</td>\n      <td>0.310</td>\n      <td>0.14</td>\n      <td>7.50</td>\n      <td>0.044</td>\n      <td>34.0</td>\n      <td>133.0</td>\n      <td>0.99550</td>\n      <td>3.22</td>\n      <td>0.50</td>\n      <td>9.500000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>6.2</td>\n      <td>0.660</td>\n      <td>0.48</td>\n      <td>1.20</td>\n      <td>0.029</td>\n      <td>29.0</td>\n      <td>75.0</td>\n      <td>0.98920</td>\n      <td>3.33</td>\n      <td>0.39</td>\n      <td>12.800000</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>6.4</td>\n      <td>0.310</td>\n      <td>0.38</td>\n      <td>2.90</td>\n      <td>0.038</td>\n      <td>19.0</td>\n      <td>102.0</td>\n      <td>0.99120</td>\n      <td>3.17</td>\n      <td>0.35</td>\n      <td>11.000000</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>6.8</td>\n      <td>0.260</td>\n      <td>0.42</td>\n      <td>1.70</td>\n      <td>0.049</td>\n      <td>41.0</td>\n      <td>122.0</td>\n      <td>0.99300</td>\n      <td>3.47</td>\n      <td>0.48</td>\n      <td>10.500000</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>7.6</td>\n      <td>0.670</td>\n      <td>0.14</td>\n      <td>1.50</td>\n      <td>0.074</td>\n      <td>25.0</td>\n      <td>168.0</td>\n      <td>0.99370</td>\n      <td>3.05</td>\n      <td>0.51</td>\n      <td>9.300000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>6.6</td>\n      <td>0.270</td>\n      <td>0.41</td>\n      <td>1.30</td>\n      <td>0.052</td>\n      <td>16.0</td>\n      <td>142.0</td>\n      <td>0.99510</td>\n      <td>3.42</td>\n      <td>0.47</td>\n      <td>10.000000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>7.0</td>\n      <td>0.250</td>\n      <td>0.32</td>\n      <td>9.00</td>\n      <td>0.046</td>\n      <td>56.0</td>\n      <td>245.0</td>\n      <td>0.99550</td>\n      <td>3.25</td>\n      <td>0.50</td>\n      <td>10.400000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>6.9</td>\n      <td>0.240</td>\n      <td>0.35</td>\n      <td>1.00</td>\n      <td>0.052</td>\n      <td>35.0</td>\n      <td>146.0</td>\n      <td>0.99300</td>\n      <td>3.45</td>\n      <td>0.44</td>\n      <td>10.000000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>7.0</td>\n      <td>0.280</td>\n      <td>0.39</td>\n      <td>8.70</td>\n      <td>0.051</td>\n      <td>32.0</td>\n      <td>141.0</td>\n      <td>0.99610</td>\n      <td>3.38</td>\n      <td>0.53</td>\n      <td>10.500000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>7.4</td>\n      <td>0.270</td>\n      <td>0.48</td>\n      <td>1.10</td>\n      <td>0.047</td>\n      <td>17.0</td>\n      <td>132.0</td>\n      <td>0.99140</td>\n      <td>3.19</td>\n      <td>0.49</td>\n      <td>11.600000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>7.2</td>\n      <td>0.320</td>\n      <td>0.36</td>\n      <td>2.00</td>\n      <td>0.033</td>\n      <td>37.0</td>\n      <td>114.0</td>\n      <td>0.99060</td>\n      <td>3.10</td>\n      <td>0.71</td>\n      <td>12.300000</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4868</th>\n      <td>5.8</td>\n      <td>0.230</td>\n      <td>0.31</td>\n      <td>4.50</td>\n      <td>0.046</td>\n      <td>42.0</td>\n      <td>124.0</td>\n      <td>0.99324</td>\n      <td>3.31</td>\n      <td>0.64</td>\n      <td>10.800000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4869</th>\n      <td>6.6</td>\n      <td>0.240</td>\n      <td>0.33</td>\n      <td>10.10</td>\n      <td>0.032</td>\n      <td>8.0</td>\n      <td>81.0</td>\n      <td>0.99626</td>\n      <td>3.19</td>\n      <td>0.51</td>\n      <td>9.800000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4870</th>\n      <td>6.1</td>\n      <td>0.320</td>\n      <td>0.28</td>\n      <td>6.60</td>\n      <td>0.021</td>\n      <td>29.0</td>\n      <td>132.0</td>\n      <td>0.99188</td>\n      <td>3.15</td>\n      <td>0.36</td>\n      <td>11.450000</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4871</th>\n      <td>5.0</td>\n      <td>0.200</td>\n      <td>0.40</td>\n      <td>1.90</td>\n      <td>0.015</td>\n      <td>20.0</td>\n      <td>98.0</td>\n      <td>0.98970</td>\n      <td>3.37</td>\n      <td>0.55</td>\n      <td>12.050000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4872</th>\n      <td>6.0</td>\n      <td>0.420</td>\n      <td>0.41</td>\n      <td>12.40</td>\n      <td>0.032</td>\n      <td>50.0</td>\n      <td>179.0</td>\n      <td>0.99622</td>\n      <td>3.14</td>\n      <td>0.60</td>\n      <td>9.700000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4873</th>\n      <td>5.7</td>\n      <td>0.210</td>\n      <td>0.32</td>\n      <td>1.60</td>\n      <td>0.030</td>\n      <td>33.0</td>\n      <td>122.0</td>\n      <td>0.99044</td>\n      <td>3.33</td>\n      <td>0.52</td>\n      <td>11.900000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4874</th>\n      <td>5.6</td>\n      <td>0.200</td>\n      <td>0.36</td>\n      <td>2.50</td>\n      <td>0.048</td>\n      <td>16.0</td>\n      <td>125.0</td>\n      <td>0.99282</td>\n      <td>3.49</td>\n      <td>0.49</td>\n      <td>10.000000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4875</th>\n      <td>7.4</td>\n      <td>0.220</td>\n      <td>0.26</td>\n      <td>1.20</td>\n      <td>0.035</td>\n      <td>18.0</td>\n      <td>97.0</td>\n      <td>0.99245</td>\n      <td>3.12</td>\n      <td>0.41</td>\n      <td>9.700000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4876</th>\n      <td>6.2</td>\n      <td>0.380</td>\n      <td>0.42</td>\n      <td>2.50</td>\n      <td>0.038</td>\n      <td>34.0</td>\n      <td>117.0</td>\n      <td>0.99132</td>\n      <td>3.36</td>\n      <td>0.59</td>\n      <td>11.600000</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4877</th>\n      <td>5.9</td>\n      <td>0.540</td>\n      <td>0.00</td>\n      <td>0.80</td>\n      <td>0.032</td>\n      <td>12.0</td>\n      <td>82.0</td>\n      <td>0.99286</td>\n      <td>3.25</td>\n      <td>0.36</td>\n      <td>8.800000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4878</th>\n      <td>6.2</td>\n      <td>0.530</td>\n      <td>0.02</td>\n      <td>0.90</td>\n      <td>0.035</td>\n      <td>6.0</td>\n      <td>81.0</td>\n      <td>0.99234</td>\n      <td>3.24</td>\n      <td>0.35</td>\n      <td>9.500000</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4879</th>\n      <td>6.6</td>\n      <td>0.340</td>\n      <td>0.40</td>\n      <td>8.10</td>\n      <td>0.046</td>\n      <td>68.0</td>\n      <td>170.0</td>\n      <td>0.99494</td>\n      <td>3.15</td>\n      <td>0.50</td>\n      <td>9.533333</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4880</th>\n      <td>6.6</td>\n      <td>0.340</td>\n      <td>0.40</td>\n      <td>8.10</td>\n      <td>0.046</td>\n      <td>68.0</td>\n      <td>170.0</td>\n      <td>0.99494</td>\n      <td>3.15</td>\n      <td>0.50</td>\n      <td>9.533333</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4881</th>\n      <td>5.0</td>\n      <td>0.235</td>\n      <td>0.27</td>\n      <td>11.75</td>\n      <td>0.030</td>\n      <td>34.0</td>\n      <td>118.0</td>\n      <td>0.99540</td>\n      <td>3.07</td>\n      <td>0.50</td>\n      <td>9.400000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4882</th>\n      <td>5.5</td>\n      <td>0.320</td>\n      <td>0.13</td>\n      <td>1.30</td>\n      <td>0.037</td>\n      <td>45.0</td>\n      <td>156.0</td>\n      <td>0.99184</td>\n      <td>3.26</td>\n      <td>0.38</td>\n      <td>10.700000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4883</th>\n      <td>4.9</td>\n      <td>0.470</td>\n      <td>0.17</td>\n      <td>1.90</td>\n      <td>0.035</td>\n      <td>60.0</td>\n      <td>148.0</td>\n      <td>0.98964</td>\n      <td>3.27</td>\n      <td>0.35</td>\n      <td>11.500000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4884</th>\n      <td>6.5</td>\n      <td>0.330</td>\n      <td>0.38</td>\n      <td>8.30</td>\n      <td>0.048</td>\n      <td>68.0</td>\n      <td>174.0</td>\n      <td>0.99492</td>\n      <td>3.14</td>\n      <td>0.50</td>\n      <td>9.600000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4885</th>\n      <td>6.6</td>\n      <td>0.340</td>\n      <td>0.40</td>\n      <td>8.10</td>\n      <td>0.046</td>\n      <td>68.0</td>\n      <td>170.0</td>\n      <td>0.99494</td>\n      <td>3.15</td>\n      <td>0.50</td>\n      <td>9.550000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4886</th>\n      <td>6.2</td>\n      <td>0.210</td>\n      <td>0.28</td>\n      <td>5.70</td>\n      <td>0.028</td>\n      <td>45.0</td>\n      <td>121.0</td>\n      <td>0.99168</td>\n      <td>3.21</td>\n      <td>1.08</td>\n      <td>12.150000</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4887</th>\n      <td>6.2</td>\n      <td>0.410</td>\n      <td>0.22</td>\n      <td>1.90</td>\n      <td>0.023</td>\n      <td>5.0</td>\n      <td>56.0</td>\n      <td>0.98928</td>\n      <td>3.04</td>\n      <td>0.79</td>\n      <td>13.000000</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4888</th>\n      <td>6.8</td>\n      <td>0.220</td>\n      <td>0.36</td>\n      <td>1.20</td>\n      <td>0.052</td>\n      <td>38.0</td>\n      <td>127.0</td>\n      <td>0.99330</td>\n      <td>3.04</td>\n      <td>0.54</td>\n      <td>9.200000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4889</th>\n      <td>4.9</td>\n      <td>0.235</td>\n      <td>0.27</td>\n      <td>11.75</td>\n      <td>0.030</td>\n      <td>34.0</td>\n      <td>118.0</td>\n      <td>0.99540</td>\n      <td>3.07</td>\n      <td>0.50</td>\n      <td>9.400000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4890</th>\n      <td>6.1</td>\n      <td>0.340</td>\n      <td>0.29</td>\n      <td>2.20</td>\n      <td>0.036</td>\n      <td>25.0</td>\n      <td>100.0</td>\n      <td>0.98938</td>\n      <td>3.06</td>\n      <td>0.44</td>\n      <td>11.800000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4891</th>\n      <td>5.7</td>\n      <td>0.210</td>\n      <td>0.32</td>\n      <td>0.90</td>\n      <td>0.038</td>\n      <td>38.0</td>\n      <td>121.0</td>\n      <td>0.99074</td>\n      <td>3.24</td>\n      <td>0.46</td>\n      <td>10.600000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4892</th>\n      <td>6.5</td>\n      <td>0.230</td>\n      <td>0.38</td>\n      <td>1.30</td>\n      <td>0.032</td>\n      <td>29.0</td>\n      <td>112.0</td>\n      <td>0.99298</td>\n      <td>3.29</td>\n      <td>0.54</td>\n      <td>9.700000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4893</th>\n      <td>6.2</td>\n      <td>0.210</td>\n      <td>0.29</td>\n      <td>1.60</td>\n      <td>0.039</td>\n      <td>24.0</td>\n      <td>92.0</td>\n      <td>0.99114</td>\n      <td>3.27</td>\n      <td>0.50</td>\n      <td>11.200000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4894</th>\n      <td>6.6</td>\n      <td>0.320</td>\n      <td>0.36</td>\n      <td>8.00</td>\n      <td>0.047</td>\n      <td>57.0</td>\n      <td>168.0</td>\n      <td>0.99490</td>\n      <td>3.15</td>\n      <td>0.46</td>\n      <td>9.600000</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4895</th>\n      <td>6.5</td>\n      <td>0.240</td>\n      <td>0.19</td>\n      <td>1.20</td>\n      <td>0.041</td>\n      <td>30.0</td>\n      <td>111.0</td>\n      <td>0.99254</td>\n      <td>2.99</td>\n      <td>0.46</td>\n      <td>9.400000</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4896</th>\n      <td>5.5</td>\n      <td>0.290</td>\n      <td>0.30</td>\n      <td>1.10</td>\n      <td>0.022</td>\n      <td>20.0</td>\n      <td>110.0</td>\n      <td>0.98869</td>\n      <td>3.34</td>\n      <td>0.38</td>\n      <td>12.800000</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4897</th>\n      <td>6.0</td>\n      <td>0.210</td>\n      <td>0.38</td>\n      <td>0.80</td>\n      <td>0.020</td>\n      <td>22.0</td>\n      <td>98.0</td>\n      <td>0.98941</td>\n      <td>3.26</td>\n      <td>0.32</td>\n      <td>11.800000</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>4898 rows \u00d7 12 columns</p>\n</div>", "text/plain": "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0               7.0             0.270         0.36           20.70      0.045   \n1               6.3             0.300         0.34            1.60      0.049   \n2               8.1             0.280         0.40            6.90      0.050   \n3               7.2             0.230         0.32            8.50      0.058   \n4               7.2             0.230         0.32            8.50      0.058   \n5               8.1             0.280         0.40            6.90      0.050   \n6               6.2             0.320         0.16            7.00      0.045   \n7               7.0             0.270         0.36           20.70      0.045   \n8               6.3             0.300         0.34            1.60      0.049   \n9               8.1             0.220         0.43            1.50      0.044   \n10              8.1             0.270         0.41            1.45      0.033   \n11              8.6             0.230         0.40            4.20      0.035   \n12              7.9             0.180         0.37            1.20      0.040   \n13              6.6             0.160         0.40            1.50      0.044   \n14              8.3             0.420         0.62           19.25      0.040   \n15              6.6             0.170         0.38            1.50      0.032   \n16              6.3             0.480         0.04            1.10      0.046   \n17              6.2             0.660         0.48            1.20      0.029   \n18              7.4             0.340         0.42            1.10      0.033   \n19              6.5             0.310         0.14            7.50      0.044   \n20              6.2             0.660         0.48            1.20      0.029   \n21              6.4             0.310         0.38            2.90      0.038   \n22              6.8             0.260         0.42            1.70      0.049   \n23              7.6             0.670         0.14            1.50      0.074   \n24              6.6             0.270         0.41            1.30      0.052   \n25              7.0             0.250         0.32            9.00      0.046   \n26              6.9             0.240         0.35            1.00      0.052   \n27              7.0             0.280         0.39            8.70      0.051   \n28              7.4             0.270         0.48            1.10      0.047   \n29              7.2             0.320         0.36            2.00      0.033   \n...             ...               ...          ...             ...        ...   \n4868            5.8             0.230         0.31            4.50      0.046   \n4869            6.6             0.240         0.33           10.10      0.032   \n4870            6.1             0.320         0.28            6.60      0.021   \n4871            5.0             0.200         0.40            1.90      0.015   \n4872            6.0             0.420         0.41           12.40      0.032   \n4873            5.7             0.210         0.32            1.60      0.030   \n4874            5.6             0.200         0.36            2.50      0.048   \n4875            7.4             0.220         0.26            1.20      0.035   \n4876            6.2             0.380         0.42            2.50      0.038   \n4877            5.9             0.540         0.00            0.80      0.032   \n4878            6.2             0.530         0.02            0.90      0.035   \n4879            6.6             0.340         0.40            8.10      0.046   \n4880            6.6             0.340         0.40            8.10      0.046   \n4881            5.0             0.235         0.27           11.75      0.030   \n4882            5.5             0.320         0.13            1.30      0.037   \n4883            4.9             0.470         0.17            1.90      0.035   \n4884            6.5             0.330         0.38            8.30      0.048   \n4885            6.6             0.340         0.40            8.10      0.046   \n4886            6.2             0.210         0.28            5.70      0.028   \n4887            6.2             0.410         0.22            1.90      0.023   \n4888            6.8             0.220         0.36            1.20      0.052   \n4889            4.9             0.235         0.27           11.75      0.030   \n4890            6.1             0.340         0.29            2.20      0.036   \n4891            5.7             0.210         0.32            0.90      0.038   \n4892            6.5             0.230         0.38            1.30      0.032   \n4893            6.2             0.210         0.29            1.60      0.039   \n4894            6.6             0.320         0.36            8.00      0.047   \n4895            6.5             0.240         0.19            1.20      0.041   \n4896            5.5             0.290         0.30            1.10      0.022   \n4897            6.0             0.210         0.38            0.80      0.020   \n\n      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n0                    45.0                 170.0  1.00100  3.00       0.45   \n1                    14.0                 132.0  0.99400  3.30       0.49   \n2                    30.0                  97.0  0.99510  3.26       0.44   \n3                    47.0                 186.0  0.99560  3.19       0.40   \n4                    47.0                 186.0  0.99560  3.19       0.40   \n5                    30.0                  97.0  0.99510  3.26       0.44   \n6                    30.0                 136.0  0.99490  3.18       0.47   \n7                    45.0                 170.0  1.00100  3.00       0.45   \n8                    14.0                 132.0  0.99400  3.30       0.49   \n9                    28.0                 129.0  0.99380  3.22       0.45   \n10                   11.0                  63.0  0.99080  2.99       0.56   \n11                   17.0                 109.0  0.99470  3.14       0.53   \n12                   16.0                  75.0  0.99200  3.18       0.63   \n13                   48.0                 143.0  0.99120  3.54       0.52   \n14                   41.0                 172.0  1.00020  2.98       0.67   \n15                   28.0                 112.0  0.99140  3.25       0.55   \n16                   30.0                  99.0  0.99280  3.24       0.36   \n17                   29.0                  75.0  0.98920  3.33       0.39   \n18                   17.0                 171.0  0.99170  3.12       0.53   \n19                   34.0                 133.0  0.99550  3.22       0.50   \n20                   29.0                  75.0  0.98920  3.33       0.39   \n21                   19.0                 102.0  0.99120  3.17       0.35   \n22                   41.0                 122.0  0.99300  3.47       0.48   \n23                   25.0                 168.0  0.99370  3.05       0.51   \n24                   16.0                 142.0  0.99510  3.42       0.47   \n25                   56.0                 245.0  0.99550  3.25       0.50   \n26                   35.0                 146.0  0.99300  3.45       0.44   \n27                   32.0                 141.0  0.99610  3.38       0.53   \n28                   17.0                 132.0  0.99140  3.19       0.49   \n29                   37.0                 114.0  0.99060  3.10       0.71   \n...                   ...                   ...      ...   ...        ...   \n4868                 42.0                 124.0  0.99324  3.31       0.64   \n4869                  8.0                  81.0  0.99626  3.19       0.51   \n4870                 29.0                 132.0  0.99188  3.15       0.36   \n4871                 20.0                  98.0  0.98970  3.37       0.55   \n4872                 50.0                 179.0  0.99622  3.14       0.60   \n4873                 33.0                 122.0  0.99044  3.33       0.52   \n4874                 16.0                 125.0  0.99282  3.49       0.49   \n4875                 18.0                  97.0  0.99245  3.12       0.41   \n4876                 34.0                 117.0  0.99132  3.36       0.59   \n4877                 12.0                  82.0  0.99286  3.25       0.36   \n4878                  6.0                  81.0  0.99234  3.24       0.35   \n4879                 68.0                 170.0  0.99494  3.15       0.50   \n4880                 68.0                 170.0  0.99494  3.15       0.50   \n4881                 34.0                 118.0  0.99540  3.07       0.50   \n4882                 45.0                 156.0  0.99184  3.26       0.38   \n4883                 60.0                 148.0  0.98964  3.27       0.35   \n4884                 68.0                 174.0  0.99492  3.14       0.50   \n4885                 68.0                 170.0  0.99494  3.15       0.50   \n4886                 45.0                 121.0  0.99168  3.21       1.08   \n4887                  5.0                  56.0  0.98928  3.04       0.79   \n4888                 38.0                 127.0  0.99330  3.04       0.54   \n4889                 34.0                 118.0  0.99540  3.07       0.50   \n4890                 25.0                 100.0  0.98938  3.06       0.44   \n4891                 38.0                 121.0  0.99074  3.24       0.46   \n4892                 29.0                 112.0  0.99298  3.29       0.54   \n4893                 24.0                  92.0  0.99114  3.27       0.50   \n4894                 57.0                 168.0  0.99490  3.15       0.46   \n4895                 30.0                 111.0  0.99254  2.99       0.46   \n4896                 20.0                 110.0  0.98869  3.34       0.38   \n4897                 22.0                  98.0  0.98941  3.26       0.32   \n\n        alcohol  quality  \n0      8.800000        6  \n1      9.500000        6  \n2     10.100000        6  \n3      9.900000        6  \n4      9.900000        6  \n5     10.100000        6  \n6      9.600000        6  \n7      8.800000        6  \n8      9.500000        6  \n9     11.000000        6  \n10    12.000000        5  \n11     9.700000        5  \n12    10.800000        5  \n13    12.400000        7  \n14     9.700000        5  \n15    11.400000        7  \n16     9.600000        6  \n17    12.800000        8  \n18    11.300000        6  \n19     9.500000        5  \n20    12.800000        8  \n21    11.000000        7  \n22    10.500000        8  \n23     9.300000        5  \n24    10.000000        6  \n25    10.400000        6  \n26    10.000000        6  \n27    10.500000        6  \n28    11.600000        6  \n29    12.300000        7  \n...         ...      ...  \n4868  10.800000        6  \n4869   9.800000        6  \n4870  11.450000        7  \n4871  12.050000        6  \n4872   9.700000        5  \n4873  11.900000        6  \n4874  10.000000        6  \n4875   9.700000        6  \n4876  11.600000        7  \n4877   8.800000        5  \n4878   9.500000        4  \n4879   9.533333        6  \n4880   9.533333        6  \n4881   9.400000        6  \n4882  10.700000        5  \n4883  11.500000        6  \n4884   9.600000        5  \n4885   9.550000        6  \n4886  12.150000        7  \n4887  13.000000        7  \n4888   9.200000        5  \n4889   9.400000        6  \n4890  11.800000        6  \n4891  10.600000        6  \n4892   9.700000        5  \n4893  11.200000        6  \n4894   9.600000        5  \n4895   9.400000        6  \n4896  12.800000        7  \n4897  11.800000        6  \n\n[4898 rows x 12 columns]"}, "execution_count": 32, "metadata": {}}], "execution_count": 32}, {"source": "", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3.5", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.5", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}